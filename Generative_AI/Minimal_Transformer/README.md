# ğŸ§  Minimal Transformer (PyTorch)

[![Python Version](https://img.shields.io/badge/Python-3.10+-blue)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red)](https://pytorch.org)


This project implements a **minimal Transformer block** (attention + MLP) from scratch in PyTorch, trained on a toy copy task.  
Itâ€™s designed as an educational resource to understand Transformer internals step by step.

---

## ğŸ“‚ Project Structure

```

Minimal\_Transformer/
â”‚â”€â”€ components/
â”‚   â”œâ”€â”€ attention.py           # Multi-head attention
â”‚   â”œâ”€â”€ feedforward.py         # Position-wise feedforward network
â”‚   â”œâ”€â”€ positional_encoding.py # Positional encodings
â”‚   â”œâ”€â”€ transformer_block.py   # Encoder/decoder block
â”‚   â”œâ”€â”€ encoder.py             # Encoder stack
â”‚   â”œâ”€â”€ decoder.py             # Decoder stack
â”‚   â””â”€â”€ transformer.py         # Full Transformer model
â”‚
â”‚â”€â”€ data.py        # Toy dataset generator (copy task)
â”‚â”€â”€ utils.py       # Helper functions (masking, etc.)
â”‚â”€â”€ train.py       # Training script
â”‚â”€â”€ test.py        # Testing script
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ README.md

```

---

## ğŸš€ Training

Run training with:

```bash
python train.py
````

This will:

* Train a minimal Transformer on a toy **copy task**.
* Save model checkpoints to `model_checkpoints/`.
* Save a **training loss plot** (`training_loss.png`).

---

## ğŸ” Testing

After training, run:

```bash
python test.py
```

This will:

* Load the trained checkpoint.
* Generate predictions on random toy inputs.
* Print input â†’ output pairs.

Example output:

```
===== Testing Model on Random Input =====
Source: [4, 5, 6, 7, 8]
Prediction: [4, 5, 6, 7, 8]
```

---

## ğŸ“Š Results

A sample training curve:

![Training Loss](training_loss.png)

---

## âš¡ Requirements

Install dependencies with:

```bash
pip install -r requirements.txt
```

---
